# ä½¿ç”¨ llama.cpp éƒ¨ç½² Chinese-LLaMA-Alpaca-2 å¹¶ä¸”è”åˆä½¿ç”¨ GPT-SoVITS æ¨ç†ç”Ÿæˆè¯­éŸ³

_æœ¬æ•™ç¨‹ä½¿ç”¨ wsl2-Ubuntu2204 è¿›è¡Œéƒ¨ç½²ï¼Œå…¶ä»–ç¯å¢ƒè¯·å‚è€ƒ[llamacpp_zh Â· ymcui/Chinese-LLaMA-Alpaca-2 Wiki (github.com)](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/wiki/llamacpp_zh)å’Œ[GPT-SoVITS/docs/cn/README.md at main Â· RVC-Boss/GPT-SoVITS (github.com)](https://github.com/RVC-Boss/GPT-SoVITS/blob/main/docs/cn/README.md)_

## Part 1ï¼šä½¿ç”¨ llama.cpp éƒ¨ç½² Chinese-LLaMA-Alpaca-2

_ä»¥ä¸‹å¼•ç”¨**[åŸæ–‡æ¡£](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/wiki/llamacpp_zh)**æ•™ç¨‹_

> ä»¥[llama.cpp å·¥å…·](https://github.com/ggerganov/llama.cpp)ä¸ºä¾‹ï¼Œä»‹ç»æ¨¡å‹é‡åŒ–å¹¶åœ¨æœ¬åœ°éƒ¨ç½²çš„è¯¦ç»†æ­¥éª¤ã€‚Windows åˆ™å¯èƒ½éœ€è¦ cmake ç­‰ç¼–è¯‘å·¥å…·çš„å®‰è£…ã€‚**æœ¬åœ°å¿«é€Ÿéƒ¨ç½²ä½“éªŒæ¨èä½¿ç”¨ç»è¿‡æŒ‡ä»¤ç²¾è°ƒçš„ Alpaca-2 æ¨¡å‹ï¼Œæœ‰æ¡ä»¶çš„æ¨èä½¿ç”¨ 6-bit æˆ–è€… 8-bit æ¨¡å‹ï¼Œæ•ˆæœæ›´ä½³ã€‚**Â  è¿è¡Œå‰è¯·ç¡®ä¿ï¼š
>
> 1. ç³»ç»Ÿåº”æœ‰`make`ï¼ˆMacOS/Linux è‡ªå¸¦ï¼‰æˆ–`cmake`ï¼ˆWindows éœ€è‡ªè¡Œå®‰è£…ï¼‰ç¼–è¯‘å·¥å…·
>
> 2. å»ºè®®ä½¿ç”¨ Python 3.10 ä»¥ä¸Šç¼–è¯‘å’Œè¿è¡Œè¯¥å·¥å…·
>
> ### Step 1: å…‹éš†å’Œç¼–è¯‘ llama.cpp[](https://github.com/ymcui/Chinese-Mixtral/wiki/llamacpp_zh#step-1-%E5%85%8B%E9%9A%86%E5%92%8C%E7%BC%96%E8%AF%91llamacpp)
>
> 1. ï¼ˆå¯é€‰ï¼‰å¦‚æœå·²ä¸‹è½½æ—§ç‰ˆä»“åº“ï¼Œå»ºè®®`git pull`æ‹‰å–æœ€æ–°ä»£ç ï¼Œ**å¹¶æ‰§è¡Œ`make clean`è¿›è¡Œæ¸…ç†**
>
> 2. æ‹‰å–æœ€æ–°ç‰ˆ llama.cpp ä»“åº“ä»£ç 
>
> ```shell
>
> $ git clone <https://github.com/ggerganov/llama.cpp>
>
> ```
>
> 3. å¯¹ llama.cpp é¡¹ç›®è¿›è¡Œç¼–è¯‘ï¼Œç”Ÿæˆ`./main`ï¼ˆç”¨äºæ¨ç†ï¼‰å’Œ`./quantize`ï¼ˆç”¨äºé‡åŒ–ï¼‰äºŒè¿›åˆ¶æ–‡ä»¶ã€‚
>
> ```shell
>
> $ make
>
> ```
>
> **Linux ç”¨æˆ·**å¦‚éœ€å¯ç”¨ GPU æ¨ç†ï¼Œåˆ™æ¨èä¸[BLASï¼ˆæˆ– cuBLAS å¦‚æœæœ‰ GPUï¼‰ä¸€èµ·ç¼–è¯‘](https://github.com/ggerganov/llama.cpp#blas-build)ï¼Œå¯ä»¥æé«˜ prompt å¤„ç†é€Ÿåº¦ã€‚ä»¥ä¸‹æ˜¯å’Œ cuBLAS ä¸€èµ·ç¼–è¯‘çš„å‘½ä»¤ï¼Œé€‚ç”¨äº NVIDIA ç›¸å…³ GPUã€‚å‚è€ƒï¼š[llama.cpp#blas-build](https://github.com/ggerganov/llama.cpp#blas-build)
>
> ```shell
>
> $ make LLAMA_CUBLAS=1
>
> ```
>
> ### Step 2: ç”Ÿæˆé‡åŒ–ç‰ˆæœ¬æ¨¡å‹[](https://github.com/ymcui/Chinese-Mixtral/wiki/llamacpp_zh#step-2-%E7%94%9F%E6%88%90%E9%87%8F%E5%8C%96%E7%89%88%E6%9C%AC%E6%A8%A1%E5%9E%8B)
>
> ï¼ˆğŸ’¡ ä¹Ÿå¯ç›´æ¥ä¸‹è½½å·²é‡åŒ–å¥½çš„ gguf æ¨¡å‹ï¼š[gguf æ¨¡å‹](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/tree/main?tab=readme-ov-file#%E5%AE%8C%E6%95%B4%E6%A8%A1%E5%9E%8B%E4%B8%8B%E8%BD%BD)ï¼‰
>
> ç›®å‰ llama.cpp å·²æ”¯æŒ`.pth`æ–‡ä»¶ä»¥åŠ huggingface æ ¼å¼`.bin`çš„è½¬æ¢ã€‚å°†å®Œæ•´æ¨¡å‹æƒé‡è½¬æ¢ä¸º GGML çš„ FP16 æ ¼å¼ï¼Œç”Ÿæˆæ–‡ä»¶è·¯å¾„ä¸º`zh-models/7B/ggml-model-f16.gguf`ã€‚è¿›ä¸€æ­¥å¯¹ FP16 æ¨¡å‹è¿›è¡Œ 4-bit é‡åŒ–ï¼Œç”Ÿæˆé‡åŒ–æ¨¡å‹æ–‡ä»¶è·¯å¾„ä¸º`zh-models/7B/ggml-model-q4_0.gguf`ã€‚
>
> ```shell
>
> $ python convert.py zh-models/7B/
> $ ./quantize ./zh-models/7B/ggml-model-f16.gguf ./zh-models/7B/ggml-model-q4_0.gguf q4_0
> ```

è¿™é‡Œä»¥ä¸‹è½½äº†[Chinese-LLaMA-Alpaca-2](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2?tab=readme-ov-file#%E5%AE%8C%E6%95%B4%E6%A8%A1%E5%9E%8B%E4%B8%8B%E8%BD%BD)é GGUF æ¨¡å‹ï¼‰ï¼š 1.è¯·å°†ä½ ä¸‹è½½å¥½çš„æ¨¡å‹æ–‡ä»¶æ”¾åœ¨ä½ çš„`llama.cpp/zh-models/7B/`å†… 2.åœ¨ llama.cpp æ–‡ä»¶å¤¹å†…è¿è¡Œä¸Šè¿°æŒ‡ä»¤`$ python convert.py zh-models/7B/`æ—¶ä½ å¯èƒ½é‡åˆ°ç±»ä¼¼å¦‚ä¸‹çš„æŠ¥é”™ï¼š

```python
$ python convert.py zh-models/7B/
Traceback (most recent call last):
  File "~/llama.cpp/convert.py", line 27, in <module>
    import numpy as np
ModuleNotFoundError: No module named 'numpy'
```

è¯·è‡ªè¡Œå®‰è£… numpy åº“æˆ–å…¶ä»–ç¼ºå¤±çš„åº“ï¼š

```python
pip install numpy
```

ç„¶åç»§ç»­æ‰§è¡Œå¹¶ä¸”é‡åŒ–æ¨¡å‹

_å¦ï¼šå¦‚æœä½ ä¸‹è½½äº†**GGUF ç‰ˆ**æ¨¡å‹ï¼Œè¯·ç›´æ¥æ”¾å…¥é‡åŒ–æ¨¡å‹æ–‡ä»¶è·¯å¾„ï¼ˆzh-models/7B/æˆ–å…¶ä»–ï¼‰ä¸‹å³å¯ã€‚_

### Step 3:åŠ è½½å¹¶å¯åŠ¨æ¨¡å‹

_ä½ å¯ä»¥å°†æœ¬é¡¹ç›®çš„`scripts/llama-cpp/chat.sh`æ‹·è´è‡³ llama.cpp çš„æ ¹ç›®å½•ã€‚_

æˆ–è¯·åœ¨ llama.cpp æ–‡ä»¶å¤¹å†…æ‰§è¡Œï¼š

```bash
vim chat.sh
```

ç‚¹å‡»`i`å¹¶å†™å…¥å¦‚ä¸‹å†…å®¹åç‚¹å‡» Esc å¹¶è¾“å…¥`:wq`ä¿å­˜é€€å‡ºï¼š

```bash
#!/bin/bash

# temporary script to chat with Chinese Alpaca-2 model
# usage: ./chat.sh alpaca2-ggml-model-path your-first-instruction

SYSTEM_PROMPT='You are a helpful assistant. ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„åŠ©æ‰‹ã€‚'
# SYSTEM_PROMPT='You are a helpful assistant. ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„åŠ©æ‰‹ã€‚è¯·ä½ æä¾›ä¸“ä¸šã€æœ‰é€»è¾‘ã€å†…å®¹çœŸå®ã€æœ‰ä»·å€¼çš„è¯¦ç»†å›å¤ã€‚' # Try this one, if you prefer longer response.
MODEL_PATH=$1
FIRST_INSTRUCTION=$2

./main -m "$MODEL_PATH" \
--color -i -c 4096 -t 8 --temp 0.5 --top_k 40 --top_p 0.9 --repeat_penalty 1.1 \
--in-prefix-bos --in-prefix ' [INST] ' --in-suffix ' [/INST]' -p \
"[INST] <<SYS>>
$SYSTEM_PROMPT
<</SYS>>

$FIRST_INSTRUCTION [/INST]"
```

ç„¶å

> ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å¯åŠ¨èŠå¤©ã€‚
>
> ```shell
>
> $ chmod +x chat.sh
> $ ./chat.sh zh-models/7B/ggml-model-q4_0.gguf 'è¯·åˆ—ä¸¾5æ¡æ–‡æ˜ä¹˜è½¦çš„å»ºè®®'
>
> ```
>
> åœ¨æç¤ºç¬¦ Â `>`Â  ä¹‹åè¾“å…¥ä½ çš„ promptï¼Œ`cmd/ctrl+c`ä¸­æ–­è¾“å‡ºï¼Œå¤šè¡Œä¿¡æ¯ä»¥`\`ä½œä¸ºè¡Œå°¾ã€‚å¦‚éœ€æŸ¥çœ‹å¸®åŠ©å’Œå‚æ•°è¯´æ˜ï¼Œè¯·æ‰§è¡Œ`./main -h`å‘½ä»¤ã€‚ä¸‹é¢ä»‹ç»ä¸€äº›å¸¸ç”¨çš„å‚æ•°ï¼š
>
> ```bash
>
> -c æ§åˆ¶ä¸Šä¸‹æ–‡çš„é•¿åº¦ï¼Œå€¼è¶Šå¤§è¶Šèƒ½å‚è€ƒæ›´é•¿çš„å¯¹è¯å†å²ï¼ˆé»˜è®¤ï¼š512ï¼‰
> -f æŒ‡å®špromptæ¨¡æ¿ï¼Œalpacaæ¨¡å‹è¯·åŠ è½½prompts/alpaca.txt
> -n æ§åˆ¶å›å¤ç”Ÿæˆçš„æœ€å¤§é•¿åº¦ï¼ˆé»˜è®¤ï¼š128ï¼‰
> -b æ§åˆ¶batch sizeï¼ˆé»˜è®¤ï¼š512ï¼‰
> -t æ§åˆ¶çº¿ç¨‹æ•°é‡ï¼ˆé»˜è®¤ï¼š8ï¼‰ï¼Œå¯é€‚å½“å¢åŠ 
> --repeat_penalty æ§åˆ¶ç”Ÿæˆå›å¤ä¸­å¯¹é‡å¤æ–‡æœ¬çš„æƒ©ç½šåŠ›åº¦
> --temp æ¸©åº¦ç³»æ•°ï¼Œå€¼è¶Šä½å›å¤çš„éšæœºæ€§è¶Šå°ï¼Œåä¹‹è¶Šå¤§
> --top_p, top_k æ§åˆ¶è§£ç é‡‡æ ·çš„ç›¸å…³å‚æ•°
>
> ```
>
> æ›´è¯¦ç»†çš„å®˜æ–¹è¯´æ˜è¯·å‚è€ƒï¼š[https://github.com/ggerganov/llama.cpp/tree/master/examples/main](https://github.com/ggerganov/llama.cpp/tree/master/examples/main)

### Step 4ï¼šæ¶è®¾ server

æ­¤å¤„çš„æ¶è®¾ server çš„åŠŸèƒ½ï¼Œæ˜¯ç”¨äº API è°ƒç”¨ã€æ¶è®¾ç®€æ˜“ demo çš„ï¼Œå¦‚æœä½ å¸Œæœ›è‡ªå·±æ¶è®¾æœåŠ¡å™¨ä¹Ÿæ˜¯ç±»ä¼¼çš„åŸç†ã€‚

> è¿è¡Œä»¥ä¸‹å‘½ä»¤å¯åŠ¨ serverï¼ŒäºŒè¿›åˆ¶æ–‡ä»¶`./server`åœ¨ llama.cpp æ ¹ç›®å½•ï¼ŒæœåŠ¡é»˜è®¤ç›‘å¬`127.0.0.1:8080`ã€‚è¿™é‡ŒæŒ‡å®šæ¨¡å‹è·¯å¾„ã€ä¸Šä¸‹æ–‡çª—å£å¤§å°ã€‚å¦‚æœéœ€è¦ä½¿ç”¨ GPU è§£ç ï¼Œä¹Ÿå¯æŒ‡å®š`-ngl`å‚æ•°ã€‚
>
> ```shell
>
> $ ./server -m ./zh-models/7B/ggml-model-q4_0.gguf -c 4096 -ngl 999
> ```

è¿™é‡Œçš„æŒ‡ä»¤å®é™…ä¸Šç”¨äº†ä¸¤ä¸ªç›¸å¯¹è·¯å¾„ï¼Œå¦‚æœä½ å¸Œæœ›æŠŠå¯åŠ¨è„šæœ¬æ”¾åœ¨é`llama.cpp`çš„å…¶ä»–ç›®å½•ä¸‹ï¼Œé‚£ä¹ˆå°†`./server`æ›¿æ¢ä¸º`xx/xx/xx/llama.cpp/server`å³å¯ï¼Œåé¢çš„`./zh-models/7B/ggml-model-q4_0.gguf`éƒ¨åˆ†åŒç†ã€‚æ­¤æ—¶å¯ä»¥å¾—åˆ°å¦‚ä¸‹è„šæœ¬ï¼ˆabc.shï¼‰æ–‡ä»¶å†…å®¹ï¼š

```bash
/home/xxx/xxx/llama.cpp/server -m /home/xxx/xxx/llama.cpp/zh-models/7B/ggml-model-q4_0.gguf -c 4096 -ngl 999
```

æ‰§è¡Œè¯¥è„šæœ¬ï¼ˆæˆ–å‘½ä»¤ï¼‰åå³å¯å¯åŠ¨æœåŠ¡ï¼š

> æœåŠ¡å¯åŠ¨åï¼Œå³å¯é€šè¿‡å¤šç§æ–¹å¼è¿›è¡Œè°ƒç”¨ï¼Œä¾‹å¦‚åˆ©ç”¨`curl`å‘½ä»¤ã€‚ä»¥ä¸‹æ˜¯ä¸€ä¸ªç¤ºä¾‹è„šæœ¬ï¼ˆåŒæ—¶å­˜æ”¾åœ¨[scripts/llamacpp/server_curl_example.sh](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/blob/main/scripts/llama-cpp/server_curl_example.sh)ï¼‰ï¼Œå°† Alpaca-2 çš„æ¨¡æ¿è¿›è¡ŒåŒ…è£…å¹¶åˆ©ç”¨`curl`å‘½ä»¤è¿›è¡Œ API è®¿é—®ã€‚
>
> ```shell
> # server_curl_example.sh
>
> SYSTEM_PROMPT='You are a helpful assistant. ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„åŠ©æ‰‹ã€‚'
> # SYSTEM_PROMPT='You are a helpful assistant. ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„åŠ©æ‰‹ã€‚è¯·ä½ >æä¾›ä¸“ä¸šã€æœ‰é€»è¾‘ã€å†…å®¹çœŸå®ã€æœ‰ä»·å€¼çš„è¯¦ç»†å›å¤ã€‚' # Try this one, if you >prefer longer response.
> INSTRUCTION=$1
> ALL_PROMPT="[INST] <<SYS>>\n$SYSTEM_PROMPT\n<</SYS>>\n\n$INSTRUCTION [/>INST]"
> CURL_DATA="{\"prompt\": \"$ALL_PROMPT\",\"n_predict\": 128}"
>
> curl --request POST \
> --url <http://localhost:8080/completion> \
> --header "Content-Type: application/json" \
> --data "$CURL_DATA"
>
> ```
>
> ä¾‹å¦‚ï¼Œæˆ‘ä»¬ç»™å‡ºä¸€ä¸ªç¤ºä¾‹æŒ‡ä»¤ã€‚
>
> ```shell
> $ bash server_curl_example.sh 'è¯·åˆ—ä¸¾5æ¡æ–‡æ˜ä¹˜è½¦çš„å»ºè®®'
> ```
>
> ç¨åè¿”å›å“åº”ç»“æœã€‚

è¿”å›çš„ç»“æœæ˜¯ json ä½“çš„ä¿¡æ¯ï¼Œå…¶å†…å®¹æ¯”è¾ƒå¤šï¼Œæˆ‘ä»¬å¸Œæœ›åªå¾—åˆ°æ–‡æœ¬åé¦ˆï¼Œé‚£ä¹ˆå°±éœ€è¦è‡ªå·±å¤„ç†ä¸€ä¸‹è¿”å›çš„å†…å®¹å¹¶è¾“å‡ºï¼ˆè¿™ä¹Ÿæ˜¯æˆ‘æ²¡æœ‰è´´åŸä½œè€…ç»™å‡ºçš„.sh æ–‡ä»¶å†…å®¹çš„åŸå› ï¼‰
é€šè¿‡åˆ†æåŸæ–‡ä»¶å¯ä»¥å¾—çŸ¥ï¼Œæˆ‘ä»¬éœ€è¦åŒ…è£… curl å‘½ä»¤å¹¶å°†è‡ªå·±çš„é—®é¢˜è¾“å…¥åˆ°æœåŠ¡å™¨ï¼Œç„¶å get å…¶å›ç­”å³å¯ï¼Œä»¥ä¸‹ä¸ºç¤ºä¾‹ä»£ç :

`inputques.py`

```python
import requests
import json
import re
# å®šä¹‰ç³»ç»Ÿæç¤º
SYSTEM_PROMPT = "You are a helpful assistant. ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„åŠ©æ‰‹ã€‚"

# è¯»å–æ–‡ä»¶ä¸­çš„å¥å­
with open("ä½ çš„é—®é¢˜æ–‡ä»¶è·¯å¾„/questions.txt", "r") as f:
    prompts = [line.strip() for line in f]

# æœåŠ¡å™¨åœ°å€å’Œç«¯å£
url = "http://localhost:8080/completion"

# ç”Ÿæˆè¯·æ±‚å¤´
headers = {
    "Content-Type": "application/json",
}

# éå†æç¤ºå¹¶å‘é€è¯·æ±‚
for prompt in prompts:
    # å®šä¹‰å®Œæ•´æç¤ºæ¨¡æ¿
    ALL_PROMPT = SYSTEM_PROMPT + prompt

    # ç”Ÿæˆè¯·æ±‚æ•°æ®
    data = {"prompt": ALL_PROMPT, "n_predict": 128}

    # å‘é€ POST è¯·æ±‚å¹¶è·å–å“åº”
    response = requests.post(url=url, headers=headers, json=data)

    # æ‰“å°å“åº”
    #print(response.text)

    # è§£æ JSON å“åº”
    response_data = json.loads(response.text)

    # æå– prompt éƒ¨åˆ†
    prompt = response_data["prompt"]

    # åŒ¹é…
    prompt = re.sub(r"You are a helpful assistant. ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„åŠ©æ‰‹ã€‚","",prompt)
    # æ‰“å° prompt
    print(prompt)
    # ä»…æ‰“å° "content" éƒ¨åˆ†
    print(response_data["content"])
    # æ‰“å¼€æ–‡ä»¶å¹¶å†™å…¥å†…å®¹
    with open("å‚¨å­˜å¤šæ¬¡å¯¹è¯å†…å®¹/answers.txt", "a") as f:
        f.write(f"prompt: {prompt}\n")
        f.write(f"content: {response_data['content']}\n")
    # å•æ¬¡æ‰“å¼€æ–‡ä»¶å¹¶å†™å…¥å†…å®¹
    with open("å‚¨å­˜å•æ¬¡å›ç­”å†…å®¹/answer.txt", "w") as f:
        f.write(f"{response_data['content']}\n")
```

## Part 2ï¼šæœ¬åœ°éƒ¨ç½² GPT-SoVITS

_æ­¤å†…å®¹ä¾æ—§ä½¿ç”¨ wsl2-Ubuntu2204 è¿›è¡Œéƒ¨ç½²ï¼Œå…¶ä»–ç¯å¢ƒè¯·å‚è€ƒ[å®˜æ–¹æ–‡æ¡£](https://github.com/RVC-Boss/GPT-SoVITS/blob/main/docs/cn/README.md)_

å®˜æ–¹æ•™ç¨‹ä¸­å¯¹äº Linux ç¯å¢ƒçš„è§£é‡Šè¾ƒä¸ºç®€ç•¥ï¼Œæœ¬æ–‡æ¡£æ—¨åœ¨è¡¥å……éƒ¨åˆ†å†…å®¹ï¼š

### Step 1:ä¸‹è½½ GPT-SoVITS æºä»£ç 

æœ¬æ•™ç¨‹ä¸æ•™å¦‚ä½•ä¸‹è½½ï¼Œç°åœ¨å‡è®¾ä½ å·²ç»ä¸‹è½½å®Œæˆå¹¶å°†æ–‡ä»¶å¤¹æ”¾åœ¨äº†ä½ çš„~/ç›®å½•ä¸‹ã€‚

### Step 2:å®‰è£… conda

å¯ä»¥æ ¹æ® conda çš„**[æ¸…åé•œåƒæº](https://link.zhihu.com/?target=https%3A//mirror.tuna.tsinghua.edu.cn/help/anaconda/)**å»è¿›è¡Œä¸‹è½½

```text
wget -c https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh

wget -c https://mirrors.bfsu.edu.cn/anaconda/miniconda/Miniconda3-latest-Linux-x86_64.sh        #æ¸…åçš„é•œåƒæºlatestçš„ç‰ˆæœ¬çš„è¯å°±æ˜¯è¯´ä»¥åä¸€ç›´ä¼šæ›´æ–°æœ€æ–°çš„ç‰ˆæœ¬
```

ä¸Šè¿°å‘½ä»¤å¾—åˆ°çš„æ˜¯.sh æ–‡ä»¶ï¼Œä½¿ç”¨å¦‚ä¸‹å‘½ä»¤å®‰è£…ï¼š

```bash
bash Miniconda3-latest-Linux-x86_64.sh
```

å…·ä½“è¿‡ç¨‹ä¸å†èµ˜è¿°ï¼Œå¯è‡ªè¡ŒæŸ¥é˜…

### Step 3:å®‰è£…å…¶ä»–

åœ¨ä¸Šè¿° cunda å®‰è£…å®Œæˆåè¯·é‡å¯å‘½ä»¤è¡Œç•Œé¢ã€‚
è¿™é‡Œè¦æ±‚å…ˆå¼€å¯ cunda ç¯å¢ƒï¼Œä»¥å…é€ æˆ GPT-SoVITS çš„é…ç½®å½±å“å…¶ä»–è½¯ä»¶è¿è¡Œ

```bash
conda create -n GPTSoVits python=3.9
conda activate GPTSoVits
```

æ­¤æ—¶ä½ çš„å‘½ä»¤è¡Œå‰åº”è¯¥ä¼šæ˜¯è¿™æ ·çš„ï¼š
![[Pasted image 20240303101259.png]]
ç„¶åè¯·è¿›å…¥ä½ ä¹‹å‰ä¸‹è½½å¥½çš„ GPT-SoVITS æ–‡ä»¶å¤¹å†…ï¼Œå¦‚æœæ­¤æ—¶ä½¿ç”¨`ls`å‘½ä»¤ï¼Œä½ å¯ä»¥åœ¨é‡Œé¢æ‰¾åˆ°ä¸¤ä¸ªæ–‡ä»¶ï¼š`install.sh`å’Œ`requirements.txt`
æ­¤æ—¶è¿è¡ŒæŒ‡ä»¤ï¼Œç­‰å¾…å®‰è£…å®Œæˆå³å¯ï¼š

```bash
bash install.sh
```

ï¼ˆå¦ï¼šå¥½åƒç”¨`pip install -i https://pypi.tuna.tsinghua.edu.cn/simple -r requirements.txt`ä¹Ÿå¯ä»¥å®‰è£…ï¼Œè¿™é‡Œçš„å›å¿†ç¼ºå¤±äº†.jpgğŸ˜­ï¼‰
å‚è€ƒæ•™ç¨‹ï¼š[MAC æ•™ç¨‹ (yuque.com)](https://www.yuque.com/baicaigongchang1145haoyuangong/ib3g1e/znoph9dtetg437xb)
ï¼ˆå¯¹ï¼Œæˆ‘æ˜¯çœ‹ç€ MAC çš„æ•™ç¨‹å®‰çš„ï¼‰
ä¸‹æ¬¡å†å¯åŠ¨ï¼Œåªéœ€è¦æ‰“å¼€ç»ˆç«¯ï¼Œå®šä½åˆ°é¡¹ç›®ç›®å½•ï¼Œè¿›å…¥ conda ç¯å¢ƒï¼Œè¿è¡Œå³å¯

```bash
cd ~/GPT-SoVITS-main/GPT-SoVITS

conda activate GPTSoVits
```

### Step 4:æ¨ç†

> åœ¨/GPT-SoVITS-main/è·¯å¾„ä¸‹ï¼Œè¿è¡Œä»¥ä¸‹å‘½ä»¤å³å¯å¯åŠ¨ webui ç•Œé¢ï¼š
>
> ```bash
> python webui.py
> ```

å½“ç„¶ï¼Œä½ æ²¡æœ‰ä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹è‚¯å®šä¼šæŠ¥é”™ï¼Œæˆ‘æ˜¯ç›´æ¥æŠŠ win ä¸‹æ•´åˆåŒ…é‡Œé¢çš„ä¸œè¥¿ä¸¢åˆ°æŠ¥é”™ç¼ºå¤±çš„æ–‡ä»¶å¤¹å†…çš„ï¼Œä½ å¯ä»¥è¿™æ ·åšï¼š

> ä» Â [GPT-SoVITS Models](https://huggingface.co/lj1995/GPT-SoVITS)Â  ä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹ï¼Œå¹¶å°†å®ƒä»¬æ”¾ç½®åœ¨ Â `GPT_SoVITS\pretrained_models`Â  ä¸­ã€‚
>
> å¯¹äº UVR5ï¼ˆäººå£°/ä¼´å¥åˆ†ç¦»å’Œæ··å“ç§»é™¤ï¼Œé™„åŠ ï¼‰ï¼Œä» Â [UVR5 Weights](https://huggingface.co/lj1995/VoiceConversionWebUI/tree/main/uvr5_weights)Â  ä¸‹è½½æ¨¡å‹ï¼Œå¹¶å°†å®ƒä»¬æ”¾ç½®åœ¨ Â `tools/uvr5/uvr5_weights`Â  ä¸­ã€‚
> ä¸­å›½åœ°åŒºç”¨æˆ·å¯ä»¥è¿›å…¥ä»¥ä¸‹é“¾æ¥å¹¶ç‚¹å‡»â€œä¸‹è½½å‰¯æœ¬â€ä¸‹è½½ä»¥ä¸Šä¸¤ä¸ªæ¨¡å‹ï¼š
>
> - [GPT-SoVITS Models](https://www.icloud.com.cn/iclouddrive/056y_Xog_HXpALuVUjscIwTtg#GPT-SoVITS_Models)
>
> - [UVR5 Weights](https://www.icloud.com.cn/iclouddrive/0bekRKDiJXboFhbfm3lM2fVbA#UVR5_Weights)
>
> å¯¹äºä¸­æ–‡è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆé™„åŠ ï¼‰ï¼Œä» Â [Damo ASR Model](https://modelscope.cn/models/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch/files),Â [Damo VAD Model](https://modelscope.cn/models/damo/speech_fsmn_vad_zh-cn-16k-common-pytorch/files), å’Œ Â [Damo Punc Model](https://modelscope.cn/models/damo/punc_ct-transformer_zh-cn-common-vocab272727-pytorch/files)Â  ä¸‹è½½æ¨¡å‹ï¼Œå¹¶å°†å®ƒä»¬æ”¾ç½®åœ¨ Â `tools/damo_asr/models`Â  ä¸­ã€‚

~~æˆ‘çš„ç›®çš„æ˜¯æ¨ç†ï¼Œåˆä¸æ˜¯è®­ç»ƒï¼Œè€Œä¸”éƒ½ç”¨å‘½ä»¤è¡Œäº†è¿˜è¦ä»€ä¹ˆ ui ç•Œé¢ ğŸ¤ª~~
åŸºäºç§ç§åŸå› ï¼ˆï¼Ÿï¼‰æˆ‘å†³å®šæ‰¾åˆ°æ¨ç†ç•Œé¢ç›´æ¥è¿è¡Œæ¨ç†å³å¯
äº‹å®ä¸Šï¼Œ[æœ‰è®¡åˆ’æ¨å‡ºä¸€ä¸ªå‘½ä»¤è¡Œä¸€é”®è®­ç»ƒå’Œæ¨ç†çš„è„šæœ¬å…¥å£ä¹ˆ Â· Issue #489 Â· RVC-Boss/GPT-SoVITS (github.com)](https://github.com/RVC-Boss/GPT-SoVITS/issues/489)å·²ç»æå‡ºäº†ç–‘é—®
é¡ºè—¤æ‘¸ç“œä¸‹ï¼Œæˆ‘å¾—åˆ°äº†å¦‚ä¸‹å†…å®¹ï¼š

- åœ¨ GPT-SoVITS-main/GPT_SoVITS å†…å­˜æœ‰äºŒçº§ç•Œé¢çš„å¯åŠ¨.py æ–‡ä»¶
- æ¨ç†ç•Œé¢çš„.py æ–‡ä»¶ä¸º`inference_webui.py`
- è¯¥æ–‡ä»¶éœ€è¦ä¾èµ– GPT-SoVITS-main æ–‡ä»¶å¤¹ä¸‹çš„å…¶ä»–å†…å®¹ï¼Œå¹¶ä¸”ä½œè€…å°†å…¶å†™æˆäº†ç›¸å¯¹è·¯å¾„

ç†æ‰€å½“ç„¶çš„çš„å°±æŠŠä»–ä» GPT-SoVITS-main/GPT_SoVITS å¤åˆ¶åˆ°äº† GPT-SoVITS-main ä¸‹é¢ã€‚å¹¶ä¸”ä½¿ç”¨å‘½ä»¤æˆåŠŸå¯åŠ¨ï¼š

```bash
python inference_webui.py
```

ä½†æ˜¯æ­¤æ—¶é—®é¢˜æ¥äº†ï¼Œæˆ‘å¦‚æœä»¿ç…§ Part1 ä¸­ç”¨ curl çš„æ–¹æ³•æ¨é€å¹¶è·å–ç»“æœï¼ŒæœåŠ¡å™¨ä¼šæŠ¥é”™ï¼š

```bash
{'detail': 'Method Not Allowed'}
```

å¾ˆå¥½ï¼Œåªèƒ½å¦å¯»ä»–æ³•ã€‚
ç„¶åæˆ‘åœ¨ GPT-SoVITS-main ä¸‹ç¿»åˆ°äº† api.py
è°¢è°¢ä½ ä½œè€…ï¼Œä¸ºä»€ä¹ˆä¸æŠŠ api è°ƒç”¨æ–¹æ³•å†™åœ¨æ–‡æ¡£é‡Œè€Œæ˜¯å†™åœ¨äº†æ–‡ä»¶é‡Œ
ï¼ˆå¦‚æœæ˜¯å› ä¸ºæˆ‘æ²¡æ‰¾åˆ° api æ–‡æ¡£è¯·æ¥å—æˆ‘çš„é“æ­‰ ğŸ˜­ï¼‰

è¿™ä¸ªæ—¶å€™å°±ç®€å•äº†ï¼Œç›´æ¥å¯åŠ¨è¿œç¨‹ç«¯å£ï¼š

```bash
python3 api.py
```

>è¿™é‡Œå¯èƒ½æŠ¥é”™ï¼š zipfile.BadZipFile: File is not a zip file
>å¯¼èˆªåˆ° zip æ–‡ä»¶çš„é¢„æœŸè·¯å¾„ï¼š/home/molin/miniconda3/envs/GPTSoVits/lib/python3.9/site-packages/nltk/data/taggers
>å¦‚æœæ–‡ä»¶ä¸¢å¤±ï¼Œè¯·ä½¿ç”¨ NLTK çš„ download() å‡½æ•°é‡æ–°ä¸‹è½½ï¼š
>```Python
>import nltk
>nltk.download('averaged_perceptron_tagger')
>```
>è¯·è°¨æ…ä½¿ç”¨ä»£ç ã€‚
>å¦‚æœæ–‡ä»¶å­˜åœ¨ï¼Œè¯·ä½¿ç”¨ 7z æˆ– unzip ç­‰å·¥å…·æ£€æŸ¥å…¶å®Œæ•´æ€§ï¼š
>```Bash
>7z t taggers/averaged_perceptron_tagger.zip
>```
>è¯·è°¨æ…ä½¿ç”¨ä»£ç ã€‚
>å¦‚æœæ–‡ä»¶æŸåï¼Œè¯·åˆ é™¤å®ƒå¹¶ä½¿ç”¨ NLTK çš„ download() å‡½æ•°é‡æ–°ä¸‹è½½ã€‚
>
>éªŒè¯ NLTK æ•°æ®è·¯å¾„æ˜¯å¦è®¾ç½®æ­£ç¡®ã€‚æ‚¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹æ–¹æ³•æ£€æŸ¥ï¼š
>```Python
>nltk.data.path
>```
>
>å¦‚æœå¿…è¦ï¼Œä½¿ç”¨ä»¥ä¸‹æ–¹æ³•è®¾ç½®è·¯å¾„ï¼š
>```Python
>nltk.data.path.append('/path/to/nltk_data')
>```

è¿™é‡Œçš„ç«¯å£å·æ˜¯ 9880ï¼Œä½¿ç”¨<http://localhost:9880/å³å¯è®¿é—®ã€‚>

è¿™é‡ŒåŒæ ·ä½¿ç”¨ curl æ–¹æ³•æ¨é€å‚æ•°å¹¶è§£æè¿”å›å€¼ï¼Œæˆ‘å·²ç»å†™æˆäº† python æ–‡ä»¶å¦‚ä¸‹ï¼š

`getvoice.py`

```python
import requests
import json

# è¯»å–æ–‡æœ¬å†…å®¹
with open("ä½ çš„answeræ–‡ä»¶å‚¨å­˜è·¯å¾„/answer.txt", "r") as f:
    text = f.read()

# å®šä¹‰è¯·æ±‚å‚æ•°
url = "http://localhost:9880/"
headers = {"Content-Type": "application/json"}
data = {
    "refer_wav_path": "ç¤ºä¾‹è¯­éŸ³ï¼Œå’Œç½‘é¡µç«¯çš„è¦æ±‚ç›¸åŒï¼Œå»ºè®®5-10s",
    "prompt_text": "è¿™æ˜¯ä½ ä¸Šé¢ç¤ºä¾‹è¯­éŸ³çš„æ–‡æœ¬",
    "prompt_language": "zh",
    "text": text,
    "text_language": "zh",
}

# å‘é€è¯·æ±‚å¹¶è·å–å“åº”
response = requests.post(url, headers=headers, data=json.dumps(data))

# å¤„ç†ç»“æœ
if response.status_code == 200:
    # æˆåŠŸ
    # è¿™é‡Œå¯ä»¥å°†éŸ³é¢‘æ•°æ®ä¿å­˜åˆ°æ–‡ä»¶
    with open("~/output.wav", "wb") as f:
        f.write(response.content)
else:
    # å¤±è´¥
    error_info = json.loads(response.content)
    print(error_info)
```

## Part 3:è”åˆä½¿ç”¨ Chinese-LLaMA-Alpaca-2 å’Œ GPT-SoVITS

æ€»ç»“ä»¥ä¸Šæ­¥éª¤å³

- åœ¨ llama.cpp ä¸­ä½¿ç”¨ bash start_sever.sh å¯åŠ¨è¿œç¨‹ç«¯å£ï¼Œ
  ä½¿ç”¨ pytohon3 inputques.py è¯»å– question.txt å†…çš„é—®é¢˜ï¼Œ
  å°†ç­”æ¡ˆå†™å…¥ answer.txtï¼ˆè¦†ç›–ï¼‰å’Œ answers.txtï¼ˆä¿å­˜ï¼‰ã€‚

- åœ¨~/GPT-SoVITS-main ä¸­å…ˆä½¿ç”¨ conda activate GPTSoVits å¯åŠ¨è™šæ‹Ÿç¯å¢ƒ
  å†ä½¿ç”¨ python api.py å¯åŠ¨è¿œç¨‹ç«¯å£ï¼Œ
  ä½¿ç”¨ python getvoice.py è¯»å–~llama.cpp/answer.txt çš„å†…å®¹å¹¶åœ¨~/ä¸‹ç”Ÿæˆ wav æ–‡ä»¶

å¤§æ¦‚å°±æ˜¯è¿™æ · ğŸ’–

æ„Ÿè°¢æ‚¨çš„é˜…è¯» ğŸ’•

æ–‡ä¸­ inputques.py å’Œ getvoice.py ä¸ºåŸåˆ›ä»£ç ï¼Œå—æœ¬æ–‡è®¸å¯ä¿æŠ¤ï¼Œå¼•ç”¨æˆ–ä¿®æ”¹è¯·æ ‡æ˜å‡ºå¤„

~~çˆ±æ¥è‡ª Markdown~~
